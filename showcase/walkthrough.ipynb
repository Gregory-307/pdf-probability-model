{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional Forecasting Pipeline\n",
    "\n",
    "**From raw data to risk-aware decisions with uncertainty quantification.**\n",
    "\n",
    "This notebook demonstrates the complete methodology:\n",
    "\n",
    "```\n",
    "Raw Data → Feature Extraction → Model Training → PDF Parameters → E[X] & Risk → Decisions\n",
    "```\n",
    "\n",
    "Traditional ML predicts point estimates. **Distributional regression** predicts entire probability distributions, enabling:\n",
    "- Uncertainty quantification at every prediction\n",
    "- Risk measures (VaR, CVaR) directly from forecasts\n",
    "- Optimal position sizing (Kelly criterion)\n",
    "- Proper evaluation via scoring rules (CRPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats as scipy_stats, optimize\n",
    "\n",
    "import temporalpdf as tpdf\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(f\"temporalpdf version: {tpdf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Load Raw Data\n",
    "\n",
    "We use real Nordic stock returns (2014-2016). In production, this would be your target variable - whatever you're forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real market data\n",
    "df = pd.read_csv('../data/sample_returns.csv')\n",
    "df['ASOFDATE'] = pd.to_datetime(df['ASOFDATE'])\n",
    "\n",
    "print(f\"Loaded {len(df):,} observations\")\n",
    "print(f\"Companies: {df['COMPANYNAME'].nunique()}\")\n",
    "print(f\"Date range: {df['ASOFDATE'].min().date()} to {df['ASOFDATE'].max().date()}\")\n",
    "print()\n",
    "print(\"Sample:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution: forward returns\n",
    "returns = df['target'].dropna().values\n",
    "\n",
    "print(\"Target Statistics (% returns):\")\n",
    "print(f\"  Mean:     {np.mean(returns):7.4f}%\")\n",
    "print(f\"  Std:      {np.std(returns):7.4f}%\")\n",
    "print(f\"  Skewness: {scipy_stats.skew(returns):7.4f}\")\n",
    "print(f\"  Kurtosis: {scipy_stats.kurtosis(returns):7.4f} (excess)\")\n",
    "print(f\"  Min:      {np.min(returns):7.2f}%\")\n",
    "print(f\"  Max:      {np.max(returns):7.2f}%\")\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(returns, bins=60, density=True, alpha=0.7, color='steelblue', edgecolor='white')\n",
    "plt.axvline(np.mean(returns), color='red', ls='--', lw=2, label=f'Mean = {np.mean(returns):.2f}%')\n",
    "plt.xlabel('Return (%)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Forward Returns')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Heavy tails and slight negative skew - typical of equity returns.\")\n",
    "print(\"A Normal distribution would underestimate tail risk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Extract Rolling Coefficients (Features)\n",
    "\n",
    "For each prediction point, we extract **distributional features** from the historical window:\n",
    "- Rolling mean (location)\n",
    "- Rolling volatility (scale)\n",
    "- Rolling skewness (asymmetry)\n",
    "- Volatility trend (is vol increasing or decreasing?)\n",
    "\n",
    "These become inputs to the model that predicts distribution parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract rolling coefficients for one company\n",
    "company = df[df['COMPANYNAME'] == df['COMPANYNAME'].iloc[0]].copy()\n",
    "company = company.sort_values('ASOFDATE').reset_index(drop=True)\n",
    "\n",
    "print(f\"Company: {company['COMPANYNAME'].iloc[0]}\")\n",
    "print(f\"Observations: {len(company)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure coefficient extraction\nconfig = tpdf.ExtractionConfig(\n    value_column='target',\n    time_column='ASOFDATE',\n    horizon=30,              # 30-day forward horizon\n    volatility_window=20,    # 20-day rolling window for vol\n)\n\n# Extract coefficients using rolling windows\nextractor = tpdf.RollingCoefficientExtractor()\ncoefficients = extractor.extract(company, config)\n\nprint(\"Extracted Coefficients:\")\nprint(coefficients[['ASOFDATE', 'mean', 'volatility', 'skewness']].dropna().head(10))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize rolling coefficients over time\ncoef_clean = coefficients.dropna()\n\nfig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n\naxes[0].plot(coef_clean['ASOFDATE'], coef_clean['mean'], 'b-', lw=1.5)\naxes[0].axhline(0, color='gray', ls='--')\naxes[0].set_ylabel('Rolling Mean (%)')\naxes[0].set_title('Rolling Coefficients Over Time')\naxes[0].grid(alpha=0.3)\n\naxes[1].plot(coef_clean['ASOFDATE'], coef_clean['volatility'], 'r-', lw=1.5)\naxes[1].set_ylabel('Rolling Volatility (%)')\naxes[1].grid(alpha=0.3)\n\naxes[2].plot(coef_clean['ASOFDATE'], coef_clean['skewness'], 'g-', lw=1.5)\naxes[2].axhline(0, color='gray', ls='--')\naxes[2].set_ylabel('Rolling Skewness')\naxes[2].set_xlabel('Date')\naxes[2].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"These rolling coefficients become FEATURES for the distributional model.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Fit Distribution Parameters (Model Training)\n",
    "\n",
    "In a full pipeline, you'd use **NGBoost** or **XGBoost with custom loss** to predict distribution parameters from features.\n",
    "\n",
    "Here we demonstrate the core idea: **fit NIG parameters via Maximum Likelihood Estimation**.\n",
    "\n",
    "The NIG (Normal Inverse Gaussian) distribution captures:\n",
    "- Heavy tails (unlike Normal)\n",
    "- Skewness (asymmetric risk)\n",
    "- Is standard in quantitative finance (Barndorff-Nielsen, 1997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_nig_mle(returns):\n",
    "    \"\"\"\n",
    "    Fit NIG distribution parameters via Maximum Likelihood.\n",
    "    \n",
    "    In production, a gradient boosting model (NGBoost, XGBoost) would\n",
    "    predict these parameters from features. MLE is the ground truth.\n",
    "    \"\"\"\n",
    "    nig = tpdf.NIG()\n",
    "    \n",
    "    # Initial guess from moments\n",
    "    x0 = np.array([\n",
    "        np.mean(returns),\n",
    "        np.log(np.std(returns)),\n",
    "        np.log(10.0),\n",
    "        0.0,\n",
    "    ])\n",
    "    \n",
    "    def neg_log_likelihood(theta):\n",
    "        mu, log_delta, log_alpha, beta_raw = theta\n",
    "        delta = np.exp(log_delta)\n",
    "        alpha = np.exp(log_alpha)\n",
    "        beta = alpha * np.tanh(beta_raw)\n",
    "        \n",
    "        try:\n",
    "            params = tpdf.NIGParameters(mu=mu, delta=delta, alpha=alpha, beta=beta)\n",
    "            pdf_vals = np.maximum(nig.pdf(returns, 0, params), 1e-300)\n",
    "            return -np.sum(np.log(pdf_vals))\n",
    "        except ValueError:\n",
    "            return 1e10\n",
    "    \n",
    "    result = optimize.minimize(neg_log_likelihood, x0, method='Nelder-Mead',\n",
    "                               options={'maxiter': 2000})\n",
    "    \n",
    "    mu, log_delta, log_alpha, beta_raw = result.x\n",
    "    delta = np.exp(log_delta)\n",
    "    alpha = np.exp(log_alpha)\n",
    "    beta = alpha * np.tanh(beta_raw)\n",
    "    \n",
    "    return tpdf.NIGParameters(mu=mu, delta=delta, alpha=alpha, beta=beta), -result.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "n_train = int(len(returns) * 0.8)\n",
    "train_returns = returns[:n_train]\n",
    "test_returns = returns[n_train:]\n",
    "\n",
    "print(f\"Train: {len(train_returns):,} observations\")\n",
    "print(f\"Test:  {len(test_returns):,} observations\")\n",
    "\n",
    "# Fit NIG on training data\n",
    "nig = tpdf.NIG()\n",
    "fitted_params, log_lik = fit_nig_mle(train_returns)\n",
    "\n",
    "print(f\"\\nFitted NIG Parameters (MLE):\")\n",
    "print(f\"  μ (location):  {fitted_params.mu:9.5f}\")\n",
    "print(f\"  δ (scale):     {fitted_params.delta:9.5f}\")\n",
    "print(f\"  α (steepness): {fitted_params.alpha:9.4f}\")\n",
    "print(f\"  β (skewness):  {fitted_params.beta:9.4f}\")\n",
    "print(f\"\\nLog-likelihood: {log_lik:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize fitted distribution vs actual data\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Histogram overlay\n",
    "x = np.linspace(train_returns.min() - 5, train_returns.max() + 5, 500)\n",
    "axes[0].hist(train_returns, bins=50, density=True, alpha=0.6, color='steelblue', \n",
    "             edgecolor='white', label='Actual returns')\n",
    "axes[0].plot(x, nig.pdf(x, 0, fitted_params), 'r-', lw=2.5, label='Fitted NIG')\n",
    "axes[0].plot(x, scipy_stats.norm.pdf(x, np.mean(train_returns), np.std(train_returns)), \n",
    "             'g--', lw=2, label='Normal (baseline)')\n",
    "axes[0].set_xlabel('Return (%)')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('Fitted NIG vs Actual Returns')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Right: Log scale (tail comparison)\n",
    "axes[1].hist(train_returns, bins=50, density=True, alpha=0.6, color='steelblue', edgecolor='white')\n",
    "axes[1].semilogy(x, nig.pdf(x, 0, fitted_params), 'r-', lw=2.5, label='Fitted NIG')\n",
    "axes[1].semilogy(x, scipy_stats.norm.pdf(x, np.mean(train_returns), np.std(train_returns)), \n",
    "                 'g--', lw=2, label='Normal')\n",
    "axes[1].set_xlabel('Return (%)')\n",
    "axes[1].set_ylabel('Density (log scale)')\n",
    "axes[1].set_title('Tail Behavior: NIG Captures Extremes')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].set_ylim(1e-5, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"NIG fits the heavy tails much better than Normal.\")\n",
    "print(\"This matters for risk: Normal would underestimate extreme losses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Construct Time-Evolving PDF\n",
    "\n",
    "The fitted parameters define a **static** distribution. For forecasting, we need the distribution to **evolve over time**:\n",
    "\n",
    "- Uncertainty grows as we forecast further ahead\n",
    "- Volatility may mean-revert to long-run levels\n",
    "- Location may drift\n",
    "\n",
    "temporalpdf supports multiple volatility evolution models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time evolution to the fitted parameters\n",
    "# Scenario: Current vol is elevated, will mean-revert to long-run\n",
    "\n",
    "long_run_vol = fitted_params.delta * 0.8  # Assume long-run is 80% of current\n",
    "\n",
    "params_evolving = tpdf.NIGParameters(\n",
    "    mu=fitted_params.mu,\n",
    "    delta=fitted_params.delta,\n",
    "    alpha=fitted_params.alpha,\n",
    "    beta=fitted_params.beta,\n",
    "    mu_drift=0.0,  # No location drift\n",
    "    volatility_model=tpdf.mean_reverting(sigma_long=long_run_vol, kappa=0.05)\n",
    ")\n",
    "\n",
    "print(\"Time-Evolving Parameters:\")\n",
    "print(f\"  Current vol (δ):  {fitted_params.delta:.4f}\")\n",
    "print(f\"  Long-run vol:     {long_run_vol:.4f}\")\n",
    "print(f\"  Mean-reversion κ: 0.05\")\n",
    "print()\n",
    "print(\"Volatility Forecast:\")\n",
    "for t in [0, 10, 20, 30, 60]:\n",
    "    vol_t = np.sqrt(nig.variance(t, params_evolving))\n",
    "    print(f\"  t={t:2d} days: vol = {vol_t:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PDF over time grid\n",
    "result = tpdf.evaluate(\n",
    "    nig, \n",
    "    params_evolving,\n",
    "    value_range=(-30, 30),\n",
    "    time_range=(0, 60),\n",
    "    value_points=200,\n",
    "    time_points=60,\n",
    ")\n",
    "\n",
    "print(f\"PDF Matrix shape: {result.pdf_matrix.shape}\")\n",
    "print(f\"Time grid: {result.time_grid[0]:.0f} to {result.time_grid[-1]:.0f} days\")\n",
    "print(f\"Value grid: {result.value_grid[0]:.1f}% to {result.value_grid[-1]:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Surface plot of time-evolving PDF\n",
    "plotter = tpdf.PDFPlotter()\n",
    "fig = plotter.surface_3d(result, title='Time-Evolving Forecast Distribution (60-Day Horizon)')\n",
    "plt.show()\n",
    "\n",
    "print(\"The PDF spreads out over time as uncertainty grows.\")\n",
    "print(\"But with mean-reversion, it stabilizes rather than exploding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap view\n",
    "fig = plotter.heatmap(result, title='PDF Heatmap: Uncertainty Grows Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Compute E[X] - Expected Value Over Time\n",
    "\n",
    "From the PDF, we compute the **expected value** at each time point.\n",
    "\n",
    "This is what a point-estimate model would predict. But we have the full distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute E[X] at each time point\n",
    "times = np.linspace(0, 60, 61)\n",
    "expected_values = [nig.mean(t, params_evolving) for t in times]\n",
    "std_devs = [np.sqrt(nig.variance(t, params_evolving)) for t in times]\n",
    "\n",
    "expected_values = np.array(expected_values)\n",
    "std_devs = np.array(std_devs)\n",
    "\n",
    "# Plot E[X] with confidence bands\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(times, expected_values, 'b-', lw=2.5, label='E[X] (expected return)')\n",
    "plt.fill_between(times, \n",
    "                 expected_values - 1.96*std_devs, \n",
    "                 expected_values + 1.96*std_devs, \n",
    "                 alpha=0.2, color='blue', label='95% CI')\n",
    "plt.fill_between(times, \n",
    "                 expected_values - std_devs, \n",
    "                 expected_values + std_devs, \n",
    "                 alpha=0.3, color='blue', label='68% CI (±1σ)')\n",
    "\n",
    "plt.axhline(0, color='gray', ls='--')\n",
    "plt.xlabel('Days Ahead')\n",
    "plt.ylabel('Expected Return (%)')\n",
    "plt.title('Expected Value Over Time with Uncertainty Bands')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xlim(0, 60)\n",
    "plt.show()\n",
    "\n",
    "print(f\"E[X] at t=0:  {expected_values[0]:.4f}%\")\n",
    "print(f\"E[X] at t=30: {expected_values[30]:.4f}%\")\n",
    "print(f\"E[X] at t=60: {expected_values[60]:.4f}%\")\n",
    "print()\n",
    "print(\"A point-estimate model gives you ONLY the blue line.\")\n",
    "print(\"Distributional regression gives you the full uncertainty bands.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Risk Measures and Decision Utilities\n",
    "\n",
    "With the full distribution, we compute **risk measures** that point predictions can't provide:\n",
    "\n",
    "| Measure | Question Answered |\n",
    "|---------|------------------|\n",
    "| **VaR** | What's the loss threshold at 5% probability? |\n",
    "| **CVaR** | If we're in the worst 5%, what's the expected loss? |\n",
    "| **Kelly** | What fraction of capital should we bet? |\n",
    "| **P(X > k)** | What's the probability of exceeding threshold k? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk measures at t=0 (current forecast)\n",
    "var_95 = tpdf.var(nig, fitted_params, alpha=0.05)\n",
    "var_99 = tpdf.var(nig, fitted_params, alpha=0.01)\n",
    "cvar_95 = tpdf.cvar(nig, fitted_params, alpha=0.05, n_samples=50000)\n",
    "cvar_99 = tpdf.cvar(nig, fitted_params, alpha=0.01, n_samples=50000)\n",
    "kelly = tpdf.kelly_fraction(nig, fitted_params)\n",
    "\n",
    "print(\"Risk Measures (from fitted distribution):\")\n",
    "print(f\"  VaR 95%:  {var_95:7.2f}% (5% chance of losing more)\")\n",
    "print(f\"  VaR 99%:  {var_99:7.2f}% (1% chance of losing more)\")\n",
    "print(f\"  CVaR 95%: {cvar_95:7.2f}% (expected loss in worst 5%)\")\n",
    "print(f\"  CVaR 99%: {cvar_99:7.2f}% (expected loss in worst 1%)\")\n",
    "print()\n",
    "print(f\"Position Sizing:\")\n",
    "print(f\"  Full Kelly:    {kelly*100:6.2f}% of capital\")\n",
    "print(f\"  Half Kelly:    {kelly*50:6.2f}% (safer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize VaR and CVaR on the distribution\n",
    "x = np.linspace(-40, 40, 1000)\n",
    "pdf = nig.pdf(x, 0, fitted_params)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(x, pdf, 'b-', lw=2.5, label='Forecast PDF')\n",
    "ax.axvline(-var_95, color='orange', ls='--', lw=2.5, label=f'VaR 95% = {var_95:.1f}%')\n",
    "\n",
    "# Shade CVaR region (tail)\n",
    "tail_mask = x <= -var_95\n",
    "ax.fill_between(x[tail_mask], pdf[tail_mask], alpha=0.3, color='red',\n",
    "                label=f'CVaR region (E = {cvar_95:.1f}%)')\n",
    "\n",
    "ax.axvline(0, color='gray', ls=':')\n",
    "ax.set_xlabel('Return (%)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Risk Measures from Distributional Forecast')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_xlim(-40, 40)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability queries\n",
    "print(\"Probability Queries:\")\n",
    "print(f\"  P(return > 0%):    {tpdf.prob_greater_than(nig, fitted_params, 0)*100:5.1f}%\")\n",
    "print(f\"  P(return > 10%):   {tpdf.prob_greater_than(nig, fitted_params, 10)*100:5.1f}%\")\n",
    "print(f\"  P(return < -10%):  {tpdf.prob_less_than(nig, fitted_params, -10)*100:5.1f}%\")\n",
    "print(f\"  P(-5% < r < 5%):   {tpdf.prob_between(nig, fitted_params, -5, 5)*100:5.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk measures OVER TIME (how do they evolve?)\n",
    "times_risk = [0, 10, 20, 30, 60]\n",
    "\n",
    "print(\"Risk Measures Over Time (with mean-reverting volatility):\")\n",
    "print(f\"{'t':>4} | {'VaR 95%':>10} | {'CVaR 95%':>10} | {'Std Dev':>10}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for t in times_risk:\n",
    "    var_t = tpdf.var(nig, params_evolving, alpha=0.05, t=t)\n",
    "    cvar_t = tpdf.cvar(nig, params_evolving, alpha=0.05, t=t, n_samples=20000)\n",
    "    std_t = np.sqrt(nig.variance(t, params_evolving))\n",
    "    print(f\"{t:>4} | {var_t:>10.2f}% | {cvar_t:>10.2f}% | {std_t:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Evaluate with Proper Scoring Rules\n",
    "\n",
    "How do you know if your distributional forecast is good? Use **proper scoring rules**.\n",
    "\n",
    "**CRPS (Continuous Ranked Probability Score)**:\n",
    "- Generalizes MAE to probabilistic forecasts\n",
    "- Lower is better\n",
    "- Same units as the target variable\n",
    "\n",
    "Reference: Gneiting & Raftery (2007). *Strictly Proper Scoring Rules.* JASA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on held-out test set\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# CRPS for NIG forecast\n",
    "crps_nig = [tpdf.crps(nig, fitted_params, y, n_samples=5000, rng=rng) for y in test_returns]\n",
    "\n",
    "# CRPS for Normal baseline (same mean, std)\n",
    "mu_baseline = np.mean(train_returns)\n",
    "sigma_baseline = np.std(train_returns)\n",
    "crps_normal = [tpdf.crps_normal(y, mu_baseline, sigma_baseline) for y in test_returns]\n",
    "\n",
    "print(f\"Out-of-Sample Evaluation (n={len(test_returns)} test observations)\")\n",
    "print(f\"\")\n",
    "print(f\"CRPS (lower is better):\")\n",
    "print(f\"  Normal baseline: {np.mean(crps_normal):.5f}\")\n",
    "print(f\"  NIG forecast:    {np.mean(crps_nig):.5f}\")\n",
    "\n",
    "improvement = (np.mean(crps_normal) - np.mean(crps_nig)) / np.mean(crps_normal) * 100\n",
    "print(f\"  Improvement:     {improvement:+.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CRPS comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram of CRPS scores\n",
    "axes[0].hist(crps_normal, bins=30, alpha=0.6, label='Normal', color='green')\n",
    "axes[0].hist(crps_nig, bins=30, alpha=0.6, label='NIG', color='blue')\n",
    "axes[0].axvline(np.mean(crps_normal), color='green', ls='--', lw=2)\n",
    "axes[0].axvline(np.mean(crps_nig), color='blue', ls='--', lw=2)\n",
    "axes[0].set_xlabel('CRPS Score')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of CRPS Scores')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Scatter: NIG vs Normal CRPS per observation\n",
    "axes[1].scatter(crps_normal, crps_nig, alpha=0.5, s=20)\n",
    "max_val = max(max(crps_normal), max(crps_nig))\n",
    "axes[1].plot([0, max_val], [0, max_val], 'r--', lw=2, label='y=x (equal)')\n",
    "axes[1].set_xlabel('CRPS (Normal)')\n",
    "axes[1].set_ylabel('CRPS (NIG)')\n",
    "axes[1].set_title('Per-Observation CRPS: Points Below Line = NIG Wins')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count wins\n",
    "nig_wins = sum(n < g for n, g in zip(crps_nig, crps_normal))\n",
    "print(f\"NIG beats Normal on {nig_wins}/{len(test_returns)} observations ({100*nig_wins/len(test_returns):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: The Complete Pipeline\n",
    "\n",
    "```\n",
    "┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐\n",
    "│   Raw Data      │ ──▶ │ Feature Extract  │ ──▶ │ Model Training  │\n",
    "│   (returns)     │     │ (rolling coefs)  │     │ (fit NIG/MLE)   │\n",
    "└─────────────────┘     └──────────────────┘     └────────┬────────┘\n",
    "                                                          │\n",
    "                                                          ▼\n",
    "┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐\n",
    "│   Decisions     │ ◀── │   Risk Measures  │ ◀── │ Time-Evolving   │\n",
    "│ (Kelly, trades) │     │ (VaR, CVaR, E[X])│     │     PDF         │\n",
    "└─────────────────┘     └──────────────────┘     └─────────────────┘\n",
    "         │\n",
    "         ▼\n",
    "┌─────────────────┐\n",
    "│   Evaluation    │\n",
    "│ (CRPS scoring)  │\n",
    "└─────────────────┘\n",
    "```\n",
    "\n",
    "### What We Demonstrated\n",
    "\n",
    "| Step | What | Why |\n",
    "|------|------|-----|\n",
    "| 1 | Load real data | Ground truth for validation |\n",
    "| 2 | Extract rolling coefficients | Features for model |\n",
    "| 3 | Fit NIG via MLE | Learn distribution parameters |\n",
    "| 4 | Construct time-evolving PDF | Uncertainty grows over horizon |\n",
    "| 5 | Compute E[X] | Point estimate with uncertainty bands |\n",
    "| 6 | Risk measures | VaR, CVaR, Kelly, probabilities |\n",
    "| 7 | CRPS evaluation | Proper scoring on held-out data |\n",
    "\n",
    "### Key Results\n",
    "\n",
    "- **NIG fits real returns better than Normal** (captures heavy tails)\n",
    "- **Risk measures directly from forecast** (not possible with point estimates)\n",
    "- **NIG improves CRPS** over Normal baseline on out-of-sample data\n",
    "\n",
    "### References\n",
    "\n",
    "- Barndorff-Nielsen (1997). Normal Inverse Gaussian Distributions. *Scand. J. Stat.*\n",
    "- Bollerslev (1986). Generalized Autoregressive Conditional Heteroskedasticity. *J. Econometrics.*\n",
    "- Gneiting & Raftery (2007). Strictly Proper Scoring Rules. *JASA.*\n",
    "- Kelly (1956). A New Interpretation of Information Rate. *Bell System Tech. J.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}