{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# XGBoost: Point Prediction vs Distribution Prediction\n",
    "\n",
    "This notebook demonstrates the core thesis of `temporalpdf`:\n",
    "\n",
    "**Pipeline 1**: XGBoost → single number (expected return) → trade if positive\n",
    "\n",
    "**Pipeline 2**: XGBoost → 4 distribution parameters → VaR/CVaR/Kelly → risk-filtered trades\n",
    "\n",
    "Same features, same model architecture. The difference: **distribution prediction gives you uncertainty quantification**, enabling sophisticated risk management that point predictions cannot provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import temporalpdf as tpdf\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (14, 5)\n",
    "\n",
    "COST_BPS = 2  # Transaction cost in basis points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Load Data & Create Features\n",
    "\n",
    "S&P 500 daily returns from yfinance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path.cwd().parent / \"data\" / \"equity_returns.csv\")\n",
    "returns = df[\"return_pct\"].values\n",
    "print(f\"S&P 500: {len(returns):,} days\")\n",
    "\n",
    "# Create features from lookback window\n",
    "lookback = 20\n",
    "X, y = [], []\n",
    "for i in range(lookback, len(returns) - 1):\n",
    "    window = returns[i-lookback:i]\n",
    "    X.append([np.mean(window), np.std(window), window[-1], window[-2],\n",
    "              np.min(window), np.max(window), np.sum(window > 0) / lookback])\n",
    "    y.append(returns[i + 1])\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Train/test split\n",
    "split = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "print(f\"Train: {len(y_train):,}, Test: {len(y_test):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Pipeline 1: XGBoost → Point Prediction\n",
    "\n",
    "Model predicts next-day return as a single number. Trading rule: go long if prediction > 0.\n",
    "\n",
    "**What you get**: A point estimate E[X].\n",
    "\n",
    "**What you're missing**: Any sense of how uncertain that estimate is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_point = GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "model_point.fit(X_train, y_train)\n",
    "pred_point = model_point.predict(X_test)\n",
    "\n",
    "decisions_p1 = pred_point > 0\n",
    "print(f\"Pipeline 1: {np.sum(decisions_p1)} / {len(y_test)} trades ({np.mean(decisions_p1):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Pipeline 2: XGBoost → Distribution Parameters\n",
    "\n",
    "Model predicts NIG distribution parameters (mu, delta, alpha, beta).\n",
    "\n",
    "**What you get**: A full probability distribution over possible outcomes.\n",
    "\n",
    "This unlocks the entire `temporalpdf` decision toolkit:\n",
    "- `tpdf.var()` - Value at Risk (worst-case loss at confidence level)\n",
    "- `tpdf.cvar()` - Conditional VaR / Expected Shortfall (expected loss in tail)\n",
    "- `tpdf.kelly_fraction()` - Optimal position sizing\n",
    "- `tpdf.prob_greater_than()` - P(return > threshold)\n",
    "- `tpdf.prob_less_than()` - P(return < threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distribution targets: fit NIG to each training window\n",
    "print(\"Fitting NIG distributions to training windows...\")\n",
    "param_targets = []\n",
    "for i in range(lookback, lookback + len(y_train)):\n",
    "    window = returns[i-lookback:i]\n",
    "    params = tpdf.fit_nig(window)\n",
    "    # Store in transformed space (log for positive params, arctanh for bounded)\n",
    "    beta_ratio = np.clip(params.beta / params.alpha, -0.99, 0.99)\n",
    "    param_targets.append([\n",
    "        params.mu,\n",
    "        np.log(params.delta),\n",
    "        np.log(params.alpha),\n",
    "        np.arctanh(beta_ratio)\n",
    "    ])\n",
    "param_targets = np.array(param_targets)\n",
    "print(f\"Created {len(param_targets)} distribution targets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model to predict distribution parameters\n",
    "model_dist = MultiOutputRegressor(\n",
    "    GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    ")\n",
    "model_dist.fit(X_train, param_targets)\n",
    "pred_params_raw = model_dist.predict(X_test)\n",
    "print(\"Distribution model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predicted parameters to NIGParameters and compute risk metrics\n",
    "nig = tpdf.NIG()\n",
    "\n",
    "decisions_p2 = []\n",
    "var_estimates = []\n",
    "cvar_estimates = []\n",
    "kelly_estimates = []\n",
    "prob_positive = []\n",
    "prob_big_loss = []  # P(loss > 2%)\n",
    "\n",
    "for mu, log_delta, log_alpha, beta_raw in pred_params_raw:\n",
    "    # Transform back to NIG parameter space\n",
    "    delta = max(np.exp(log_delta), 0.01)\n",
    "    alpha = max(np.exp(log_alpha), 0.1)\n",
    "    beta = np.clip(alpha * np.tanh(beta_raw), -alpha + 0.01, alpha - 0.01)\n",
    "    \n",
    "    params = tpdf.NIGParameters(mu=mu, delta=delta, alpha=alpha, beta=beta)\n",
    "    \n",
    "    # Use library functions for risk metrics\n",
    "    var_5 = tpdf.var(nig, params, alpha=0.05)     # 95% VaR (positive = loss)\n",
    "    cvar_5 = tpdf.cvar(nig, params, alpha=0.05)   # Expected Shortfall\n",
    "    kelly = tpdf.kelly_fraction(nig, params)       # Optimal position size\n",
    "    p_pos = tpdf.prob_greater_than(nig, params, 0.0)  # P(return > 0)\n",
    "    p_loss = tpdf.prob_less_than(nig, params, -2.0)   # P(return < -2%)\n",
    "    \n",
    "    var_estimates.append(var_5)\n",
    "    cvar_estimates.append(cvar_5)\n",
    "    kelly_estimates.append(kelly)\n",
    "    prob_positive.append(p_pos)\n",
    "    prob_big_loss.append(p_loss)\n",
    "    \n",
    "    # Decision rule: E[return] > 0 AND VaR(5%) < 2%\n",
    "    expected = nig.mean(0.0, params)\n",
    "    decisions_p2.append(expected > 0 and var_5 < 2.0)\n",
    "\n",
    "decisions_p2 = np.array(decisions_p2)\n",
    "var_estimates = np.array(var_estimates)\n",
    "cvar_estimates = np.array(cvar_estimates)\n",
    "kelly_estimates = np.array(kelly_estimates)\n",
    "prob_positive = np.array(prob_positive)\n",
    "prob_big_loss = np.array(prob_big_loss)\n",
    "\n",
    "print(f\"Pipeline 2: {np.sum(decisions_p2)} / {len(y_test)} trades ({np.mean(decisions_p2):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe(returns):\n",
    "    if len(returns) == 0 or np.std(returns) == 0:\n",
    "        return 0\n",
    "    return np.mean(returns) / np.std(returns) * np.sqrt(252)\n",
    "\n",
    "def bootstrap_ci(returns, n=1000):\n",
    "    if len(returns) == 0:\n",
    "        return (0, 0)\n",
    "    rng = np.random.default_rng(42)\n",
    "    sharpes = [sharpe(rng.choice(returns, len(returns), replace=True)) for _ in range(n)]\n",
    "    return np.percentile(sharpes, 2.5), np.percentile(sharpes, 97.5)\n",
    "\n",
    "cost = COST_BPS / 100\n",
    "strat_bh = y_test\n",
    "strat_p1 = np.where(decisions_p1, y_test - cost, 0)\n",
    "strat_p2 = np.where(decisions_p2, y_test - cost, 0)\n",
    "\n",
    "sharpe_bh, ci_bh = sharpe(strat_bh), bootstrap_ci(strat_bh)\n",
    "sharpe_p1, ci_p1 = sharpe(strat_p1), bootstrap_ci(strat_p1)\n",
    "sharpe_p2, ci_p2 = sharpe(strat_p2), bootstrap_ci(strat_p2)\n",
    "\n",
    "print(\"STRATEGY COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Strategy':<25} {'Trades':>8} {'PnL':>10} {'Sharpe':>10} {'95% CI':>16}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Buy & Hold':<25} {len(y_test):>8} {np.sum(strat_bh):>+9.1f}% {sharpe_bh:>10.2f} [{ci_bh[0]:.2f}, {ci_bh[1]:.2f}]\")\n",
    "print(f\"{'XGBoost -> Point':<25} {np.sum(decisions_p1):>8} {np.sum(strat_p1):>+9.1f}% {sharpe_p1:>10.2f} [{ci_p1[0]:.2f}, {ci_p1[1]:.2f}]\")\n",
    "print(f\"{'XGBoost -> Distribution':<25} {np.sum(decisions_p2):>8} {np.sum(strat_p2):>+9.1f}% {sharpe_p2:>10.2f} [{ci_p2[0]:.2f}, {ci_p2[1]:.2f}]\")\n",
    "print(\"-\" * 70)\n",
    "if sharpe_p1 != 0:\n",
    "    print(f\"\\nDistribution vs Point: {(sharpe_p2/sharpe_p1 - 1)*100:+.0f}% Sharpe improvement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cumulative PnL\n",
    "ax = axes[0]\n",
    "days = np.arange(len(y_test))\n",
    "ax.plot(days, np.cumsum(strat_bh), 'k-', alpha=0.5, lw=1.5, label=f'Buy & Hold ({sharpe_bh:.2f})')\n",
    "ax.plot(days, np.cumsum(strat_p1), 'r-', lw=2, label=f'Point Prediction ({sharpe_p1:.2f})')\n",
    "ax.plot(days, np.cumsum(strat_p2), 'b-', lw=2, label=f'Distribution ({sharpe_p2:.2f})')\n",
    "ax.axhline(0, color='gray', ls=':', lw=1)\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('Cumulative PnL (%)')\n",
    "ax.set_title('Cumulative Returns (Sharpe in legend)', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Sharpe comparison\n",
    "ax = axes[1]\n",
    "x = [0, 1, 2]\n",
    "bars = ax.bar(x, [sharpe_bh, sharpe_p1, sharpe_p2], color=['gray', 'red', 'blue'], alpha=0.7)\n",
    "ax.errorbar(x, [sharpe_bh, sharpe_p1, sharpe_p2],\n",
    "            yerr=[[sharpe_bh-ci_bh[0], sharpe_p1-ci_p1[0], sharpe_p2-ci_p2[0]],\n",
    "                  [ci_bh[1]-sharpe_bh, ci_p1[1]-sharpe_p1, ci_p2[1]-sharpe_p2]],\n",
    "            fmt='none', color='black', capsize=5)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Buy & Hold', 'Point\\nPrediction', 'Distribution\\nPrediction'])\n",
    "ax.set_ylabel('Annualized Sharpe')\n",
    "ax.set_title('Risk-Adjusted Returns (95% CI)', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "for i, s in enumerate([sharpe_bh, sharpe_p1, sharpe_p2]):\n",
    "    ax.text(i, s + 0.15, f'{s:.2f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparison.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Why Distribution Prediction Wins: The VaR Filter\n",
    "\n",
    "The key insight: Pipeline 1 sees \"positive expected return\" and trades.\n",
    "\n",
    "Pipeline 2 sees the full distribution and asks:\n",
    "- What's my VaR? (How much could I lose?)\n",
    "- What's my CVaR? (If things go wrong, how bad?)\n",
    "- What's the probability of a large loss?\n",
    "\n",
    "It rejects trades where expected return is positive but risk is unacceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Days where Pipeline 1 trades but Pipeline 2 doesn't (rejected by VaR filter)\n",
    "rejected = decisions_p1 & ~decisions_p2\n",
    "kept = decisions_p2\n",
    "\n",
    "print(\"VaR FILTER ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDays rejected by VaR filter: {np.sum(rejected)}\")\n",
    "if np.sum(rejected) > 0:\n",
    "    print(f\"  Mean actual return: {np.mean(y_test[rejected]):+.3f}%\")\n",
    "    print(f\"  Std actual return:  {np.std(y_test[rejected]):.3f}%\")\n",
    "    print(f\"  Mean VaR estimate:  {np.mean(var_estimates[rejected]):.3f}%\")\n",
    "    print(f\"  Mean CVaR estimate: {np.mean(cvar_estimates[rejected]):.3f}%\")\n",
    "\n",
    "print(f\"\\nDays kept by Pipeline 2: {np.sum(kept)}\")\n",
    "if np.sum(kept) > 0:\n",
    "    print(f\"  Mean actual return: {np.mean(y_test[kept]):+.3f}%\")\n",
    "    print(f\"  Std actual return:  {np.std(y_test[kept]):.3f}%\")\n",
    "    print(f\"  Mean VaR estimate:  {np.mean(var_estimates[kept]):.3f}%\")\n",
    "    print(f\"  Mean CVaR estimate: {np.mean(cvar_estimates[kept]):.3f}%\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"The VaR filter removes high-volatility days with worse risk-adjusted returns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Rich Risk Metrics from Distribution Prediction\n",
    "\n",
    "Distribution prediction unlocks queries that are impossible with point predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# VaR distribution\n",
    "ax = axes[0, 0]\n",
    "ax.hist(var_estimates, bins=50, alpha=0.7, color='red', edgecolor='black')\n",
    "ax.axvline(2.0, color='black', ls='--', lw=2, label='VaR threshold (2%)')\n",
    "ax.set_xlabel('5% VaR (%)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Predicted VaR', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# CVaR vs VaR (CVaR is always >= VaR)\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(var_estimates, cvar_estimates, alpha=0.3, s=10)\n",
    "ax.plot([0, max(var_estimates)], [0, max(var_estimates)], 'k--', label='CVaR = VaR')\n",
    "ax.set_xlabel('VaR (%)')\n",
    "ax.set_ylabel('CVaR (%)')\n",
    "ax.set_title('CVaR vs VaR (CVaR captures tail severity)', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Kelly fraction distribution\n",
    "ax = axes[1, 0]\n",
    "kelly_clipped = np.clip(kelly_estimates, -5, 5)  # Clip for visualization\n",
    "ax.hist(kelly_clipped, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "ax.axvline(0, color='black', ls='-', lw=1)\n",
    "ax.axvline(0.5, color='blue', ls='--', lw=2, label='Half-Kelly example')\n",
    "ax.set_xlabel('Kelly Fraction')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Optimal Position Sizing (Kelly Criterion)', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# P(return > 0) vs actual outcome\n",
    "ax = axes[1, 1]\n",
    "correct = (prob_positive > 0.5) == (y_test > 0)\n",
    "ax.scatter(prob_positive[correct], y_test[correct], alpha=0.3, s=10, c='green', label='Correct')\n",
    "ax.scatter(prob_positive[~correct], y_test[~correct], alpha=0.3, s=10, c='red', label='Wrong')\n",
    "ax.axhline(0, color='black', ls='-', lw=1)\n",
    "ax.axvline(0.5, color='black', ls='--', lw=1)\n",
    "ax.set_xlabel('P(return > 0)')\n",
    "ax.set_ylabel('Actual Return (%)')\n",
    "ax.set_title('Probability Calibration', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('risk_metrics.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Alternative Decision Rules\n",
    "\n",
    "With distribution prediction, we can design sophisticated decision rules.\n",
    "Let's compare different risk thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different VaR thresholds\n",
    "thresholds = [1.0, 1.5, 2.0, 2.5, 3.0, 4.0]\n",
    "\n",
    "print(\"SENSITIVITY TO VAR THRESHOLD\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'VaR Threshold':>15} {'Trades':>10} {'PnL':>10} {'Sharpe':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Recompute expected returns using nig.mean()\n",
    "expected_returns = []\n",
    "for mu, log_delta, log_alpha, beta_raw in pred_params_raw:\n",
    "    delta = max(np.exp(log_delta), 0.01)\n",
    "    alpha = max(np.exp(log_alpha), 0.1)\n",
    "    beta = np.clip(alpha * np.tanh(beta_raw), -alpha + 0.01, alpha - 0.01)\n",
    "    params = tpdf.NIGParameters(mu=mu, delta=delta, alpha=alpha, beta=beta)\n",
    "    expected_returns.append(nig.mean(0.0, params))\n",
    "expected_returns = np.array(expected_returns)\n",
    "\n",
    "for thresh in thresholds:\n",
    "    decisions = (expected_returns > 0) & (var_estimates < thresh)\n",
    "    strat = np.where(decisions, y_test - cost, 0)\n",
    "    s = sharpe(strat)\n",
    "    print(f\"{thresh:>14.1f}% {np.sum(decisions):>10} {np.sum(strat):>+9.1f}% {s:>10.2f}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(\"Note: Tighter thresholds = fewer trades but better risk-adjusted returns (up to a point)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Kelly-Weighted Returns\n",
    "\n",
    "Instead of binary trade/no-trade, use Kelly fraction for position sizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kelly-weighted strategy: size positions by Kelly fraction\n",
    "# Cap at 1.0 (no leverage) and use half-Kelly for safety\n",
    "kelly_weights = np.clip(kelly_estimates * 0.5, 0, 1)  # Half-Kelly, long only, max 100%\n",
    "\n",
    "# Only trade when VaR is acceptable\n",
    "var_filter = var_estimates < 2.0\n",
    "kelly_weights_filtered = np.where(var_filter, kelly_weights, 0)\n",
    "\n",
    "strat_kelly = kelly_weights_filtered * (y_test - cost)\n",
    "sharpe_kelly = sharpe(strat_kelly)\n",
    "ci_kelly = bootstrap_ci(strat_kelly)\n",
    "\n",
    "print(\"KELLY-WEIGHTED STRATEGY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Half-Kelly, VaR<2% filter, long-only, max 100% position\")\n",
    "print(f\"\\nTrades (non-zero weight): {np.sum(kelly_weights_filtered > 0)}\")\n",
    "print(f\"Mean position size: {np.mean(kelly_weights_filtered[kelly_weights_filtered > 0]):.1%}\")\n",
    "print(f\"PnL: {np.sum(strat_kelly):+.1f}%\")\n",
    "print(f\"Sharpe: {sharpe_kelly:.2f} [{ci_kelly[0]:.2f}, {ci_kelly[1]:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Pipeline | Output | Decision Rule | Risk Metrics |\n",
    "|----------|--------|---------------|-------------|\n",
    "| Point | E[return] | Trade if E[X] > 0 | None |\n",
    "| Distribution | (μ, δ, α, β) | Trade if E[X] > 0 AND VaR < 2% | VaR, CVaR, Kelly, P(loss) |\n",
    "\n",
    "**Key insight**: Predicting a distribution instead of a point gives you:\n",
    "\n",
    "1. **VaR** - Maximum expected loss at confidence level (tpdf.var)\n",
    "2. **CVaR** - Expected loss given you're in the tail (tpdf.cvar)  \n",
    "3. **Kelly** - Optimal position sizing based on edge and variance (tpdf.kelly_fraction)\n",
    "4. **Probabilities** - P(return > x), P(loss > y) for any threshold (tpdf.prob_greater_than, tpdf.prob_less_than)\n",
    "\n",
    "This enables risk-aware trading that point predictions cannot achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b78vf4gr1",
   "source": "## Systematic Validation: Barrier Probability Experiments\n\nThe trading example above is one use case. To validate `temporalpdf` more broadly, we ran a systematic comparison across **12 configurations**:\n\n- **Horizons**: 10, 20, 30 days\n- **Barriers**: 3%, 5% (probability that cumulative return exceeds barrier)\n- **Features**: 8 (basic), 32 (extended with tail/autocorrelation features)\n\n**Task**: Predict P(max cumulative return over horizon ≥ barrier)\n\n| Pipeline | Method |\n|----------|--------|\n| P1 | XGBoost Classifier → P(hit) directly |\n| P2 | XGBoost → Distribution Parameters → Monte Carlo → P(hit) |\n\n**Evaluation**: Brier Score (lower = better probabilistic accuracy)",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}